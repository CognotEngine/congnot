from typing import Dict, Any, Optional, List
import json

from core.node_registry import register_node
from core.base_node import BaseNode, text_area, text_input, slider
import core.model_cache_manager as model_cache_manager

STORYBOARD_AVAILABLE = None
qwen_model = None
qwen_tokenizer = None


def init_storyboard_dependencies():
    """Initialize storyboard generation dependencies"""
    global STORYBOARD_AVAILABLE, qwen_model, qwen_tokenizer

    if STORYBOARD_AVAILABLE is not None:
        return STORYBOARD_AVAILABLE

    try:
        # ä½¿ç”¨ç°æœ‰çš„Qwenä¾èµ–
        from core.qwen_nodes import init_qwen_dependencies, QWEN_AVAILABLE, qwen_text_model, qwen_text_tokenizer
        
        if not init_qwen_dependencies():
            STORYBOARD_AVAILABLE = False
            return False
            
        qwen_model = qwen_text_model
        qwen_tokenizer = qwen_text_tokenizer
        STORYBOARD_AVAILABLE = True
        return True
    except ImportError as e:
        print(f"Storyboard dependencies not installed: {e}")
        STORYBOARD_AVAILABLE = False
        return False


@register_node(
    name="storyboard_generator",
    description="Generate storyboard prompts using Qwen LLM",
    category="video",
    icon="ğŸ“–"
)
class StoryboardGeneratorNode(BaseNode):

    class Inputs(BaseNode.Inputs):
        script_text: str = text_area(
            default="è¿™æ˜¯ä¸€ä¸ªå…³äºäººå·¥æ™ºèƒ½çš„æ•…äº‹ï¼Œæè¿°äº†ä¸€ä¸ªæœºå™¨äººå¸®åŠ©äººç±»çš„åœºæ™¯ã€‚",
            description="Script text for storyboard generation"
        )
        style_guide: str = text_area(
            default="ç°ä»£ç§‘æŠ€é£æ ¼ï¼Œæ˜äº®çš„è‰²å½©ï¼Œé«˜å¯¹æ¯”åº¦ï¼Œç»†èŠ‚ä¸°å¯Œ",
            description="Style guide for the generated images"
        )
        scene_count: int = slider(
            default=6,
            min=1,
            max=12,
            step=1,
            description="Number of scenes to generate"
        )
        max_prompt_length: int = slider(
            default=100,
            min=50,
            max=200,
            step=10,
            description="Maximum length of each prompt"
        )
        qwen_seed: str = text_input(
            default=None,
            description="Random seed for Qwen generation"
        )

    class Outputs(BaseNode.Outputs):
        image_prompts: List[str]
        audio_prompts: List[str]
        storyboard_json: str
        success: bool
        message: str

    def __call__(self, script_text: str = "è¿™æ˜¯ä¸€ä¸ªå…³äºäººå·¥æ™ºèƒ½çš„æ•…äº‹",
                 style_guide: str = "ç°ä»£ç§‘æŠ€é£æ ¼",
                 scene_count: int = 6,
                 max_prompt_length: int = 100,
                 qwen_seed: Optional[int] = None) -> dict:

        if not init_storyboard_dependencies():
            return {
                "image_prompts": [],
                "audio_prompts": [],
                "storyboard_json": "",
                "success": False,
                "message": "Storyboard dependencies not installed"
            }

        try:
            # æ„å»ºæç¤ºè¯æ¨¡æ¿
            prompt_template = f"""
            ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„è§†é¢‘å¯¼æ¼”å’Œç¼–å‰§ï¼Œè¯·æ ¹æ®ä»¥ä¸‹å‰§æœ¬å’Œé£æ ¼æŒ‡å—ï¼Œä¸º{scene_count}ä¸ªè¿ç»­çš„åˆ†é•œç”Ÿæˆè¯¦ç»†çš„å›¾åƒæç¤ºè¯å’ŒéŸ³é¢‘æç¤ºè¯ã€‚
            
            å‰§æœ¬å†…å®¹ï¼š
            {script_text}
            
            é£æ ¼æŒ‡å—ï¼š
            {style_guide}
            
            è¦æ±‚ï¼š
            1. ç”Ÿæˆ{scene_count}ä¸ªåˆ†é•œï¼Œæ¯ä¸ªåˆ†é•œåŒ…å«å›¾åƒæç¤ºè¯å’ŒéŸ³é¢‘æç¤ºè¯
            2. å›¾åƒæç¤ºè¯è¦è¯¦ç»†æè¿°åœºæ™¯ã€è§’è‰²ã€åŠ¨ä½œã€æ„å›¾ã€å…‰çº¿ç­‰è§†è§‰å…ƒç´ ï¼Œé•¿åº¦ä¸è¶…è¿‡{max_prompt_length}ä¸ªå­—ç¬¦
            3. éŸ³é¢‘æç¤ºè¯è¦æè¿°é€‚åˆè¯¥åœºæ™¯çš„èƒŒæ™¯éŸ³ä¹ã€éŸ³æ•ˆç­‰ï¼Œé•¿åº¦ä¸è¶…è¿‡50ä¸ªå­—ç¬¦
            4. æ‰€æœ‰åˆ†é•œè¦è¿è´¯ï¼Œå½¢æˆå®Œæ•´çš„æ•…äº‹æµç¨‹
            5. è¾“å‡ºæ ¼å¼å¿…é¡»æ˜¯JSONï¼ŒåŒ…å«ä¸¤ä¸ªæ•°ç»„ï¼šimage_promptså’Œaudio_prompts
            6. ç¡®ä¿JSONæ ¼å¼æ­£ç¡®ï¼Œä¸è¦åŒ…å«å…¶ä»–è§£é‡Šæ€§æ–‡å­—
            
            è¾“å‡ºç¤ºä¾‹ï¼š
            {{
                "image_prompts": [
                    "æ˜äº®çš„å®éªŒå®¤é‡Œï¼Œä¸€ä¸ªæœºå™¨äººç«™åœ¨ç§‘å­¦å®¶æ—è¾¹ï¼Œå¾®ç¬‘ç€å±•ç¤ºä¸€ä¸ªå‘æ˜",
                    "æœºå™¨äººåœ¨æˆ·å¤–å¸®åŠ©è€äººè¿‡é©¬è·¯ï¼Œå‘¨å›´æ˜¯ç»¿æ ‘æˆè«çš„è¡—é“"
                ],
                "audio_prompts": [
                    "è½»å¿«çš„ç§‘æŠ€èƒŒæ™¯éŸ³ä¹ï¼Œé”®ç›˜æ•²å‡»å£°",
                    "æ¸©é¦¨çš„å¼¦ä¹ï¼Œé¸Ÿé¸£å£°"
                ]
            }}
            """

            # ä½¿ç”¨Qwenæ¨¡å‹ç”Ÿæˆå†…å®¹
            from core.qwen_nodes import DEVICE
            import torch

            # è®¾ç½®ç§å­
            if qwen_seed is not None:
                torch.manual_seed(qwen_seed)
                if torch.cuda.is_available():
                    torch.cuda.manual_seed_all(qwen_seed)

            # ä½¿ç”¨æ¨¡å‹ç¼“å­˜ç®¡ç†å™¨è·å–æˆ–åŠ è½½æ¨¡å‹
            global qwen_model, qwen_tokenizer
            if qwen_model is None or qwen_tokenizer is None:
                qwen_model = model_cache_manager.get_model("qwen_text_model")
                qwen_tokenizer = model_cache_manager.get_model("qwen_text_tokenizer")

            # ç”Ÿæˆå†…å®¹
            inputs = qwen_tokenizer(prompt_template, return_tensors="pt").to(DEVICE)
            outputs = qwen_model.generate(
                **inputs,
                max_new_tokens=1024,
                temperature=0.7,
                top_p=0.9,
                do_sample=True
            )

            # è§£ç ç”Ÿæˆçš„å†…å®¹
            response = qwen_tokenizer.decode(outputs[0], skip_special_tokens=True)

            # æå–JSONéƒ¨åˆ†
            json_start = response.find("{")
            json_end = response.rfind("}") + 1
            if json_start == -1 or json_end == 0:
                raise ValueError("Could not find valid JSON in model response")
            
            json_response = response[json_start:json_end]
            storyboard_data = json.loads(json_response)

            # éªŒè¯è¾“å‡ºæ ¼å¼
            if "image_prompts" not in storyboard_data or "audio_prompts" not in storyboard_data:
                raise ValueError("Invalid output format: missing image_prompts or audio_prompts")
                
            if len(storyboard_data["image_prompts"]) != scene_count or len(storyboard_data["audio_prompts"]) != scene_count:
                raise ValueError(f"Expected {scene_count} prompts per type, got {len(storyboard_data['image_prompts'])} image prompts and {len(storyboard_data['audio_prompts'])} audio prompts")

            return {
                "image_prompts": storyboard_data["image_prompts"],
                "audio_prompts": storyboard_data["audio_prompts"],
                "storyboard_json": json.dumps(storyboard_data),
                "success": True,
                "message": "Storyboard generated successfully"
            }

        except Exception as e:
            print(f"Error generating storyboard: {e}")
            return {
                "image_prompts": [],
                "audio_prompts": [],
                "storyboard_json": "",
                "success": False,
                "message": f"Error generating storyboard: {str(e)}"
            }


@register_node(
    name="video_scene_generator",
    description="Generate individual video scenes with independent seed control",
    category="video",
    icon="ğŸ¬"
)
class VideoSceneGeneratorNode(BaseNode):

    class Inputs(BaseNode.Inputs):
        image_prompt: str = text_area(
            default="",
            description="Image prompt for the video scene"
        )
        video_model: str = text_input(
            default="wang22_video_v1",
            description="Video model to use"
        )
        lora_strength: float = slider(
            default=0.7,
            min=0.0,
            max=1.0,
            step=0.1,
            description="LoRA strength"
        )
        denoise: float = slider(
            default=0.8,
            min=0.0,
            max=1.0,
            step=0.1,
            description="Denoise strength"
        )
        sampler_seed: str = text_input(
            default=None,
            description="Random seed for video generation"
        )

    class Outputs(BaseNode.Outputs):
        video_path: str
        success: bool
        message: str

    def __call__(self, image_prompt: str = "",
                 video_model: str = "wang22_video_v1",
                 lora_strength: float = 0.7,
                 denoise: float = 0.8,
                 sampler_seed: Optional[int] = None) -> dict:

        try:
            # è¿™é‡Œåº”è¯¥é›†æˆå®é™…çš„è§†é¢‘ç”Ÿæˆæ¨¡å‹
            # ç›®å‰ä½¿ç”¨å ä½ç¬¦å®ç°
            import os
            import tempfile
            
            # åˆ›å»ºä¸€ä¸ªä¸´æ—¶è§†é¢‘æ–‡ä»¶è·¯å¾„ï¼ˆå®é™…åº”ç”¨ä¸­åº”è¯¥ä½¿ç”¨çœŸå®çš„è§†é¢‘ç”Ÿæˆï¼‰
            temp_dir = tempfile.gettempdir()
            video_filename = f"scene_{sampler_seed or 'temp'}.mp4"
            video_path = os.path.join(temp_dir, video_filename)
            
            # æ¨¡æ‹Ÿè§†é¢‘ç”Ÿæˆè¿‡ç¨‹
            # åœ¨å®é™…åº”ç”¨ä¸­ï¼Œè¿™é‡Œåº”è¯¥è°ƒç”¨è§†é¢‘ç”Ÿæˆæ¨¡å‹
            print(f"Generating video for prompt: {image_prompt[:50]}...")
            
            return {
                "video_path": video_path,
                "success": True,
                "message": f"Video generated successfully at {video_path}"
            }

        except Exception as e:
            print(f"Error generating video scene: {e}")
            return {
                "video_path": "",
                "success": False,
                "message": f"Error generating video scene: {str(e)}"
            }


@register_node(
    name="audio_scene_generator",
    description="Generate audio for video scenes with independent seed control",
    category="video",
    icon="ğŸ”Š"
)
class AudioSceneGeneratorNode(BaseNode):

    class Inputs(BaseNode.Inputs):
        audio_prompt: str = text_area(
            default="",
            description="Audio prompt for the scene"
        )
        audio_model: str = text_input(
            default="stable_audio_v1",
            description="Audio model to use"
        )
        duration: int = slider(
            default=5,
            min=1,
            max=30,
            step=1,
            description="Audio duration in seconds"
        )
        audio_seed: str = text_input(
            default=None,
            description="Random seed for audio generation"
        )

    class Outputs(BaseNode.Outputs):
        audio_path: str
        success: bool
        message: str

    def __call__(self, audio_prompt: str = "",
                 audio_model: str = "stable_audio_v1",
                 duration: int = 5,
                 audio_seed: Optional[int] = None) -> dict:

        try:
            # è¿™é‡Œåº”è¯¥é›†æˆå®é™…çš„éŸ³é¢‘ç”Ÿæˆæ¨¡å‹
            # ç›®å‰ä½¿ç”¨å ä½ç¬¦å®ç°
            import os
            import tempfile
            
            # åˆ›å»ºä¸€ä¸ªä¸´æ—¶éŸ³é¢‘æ–‡ä»¶è·¯å¾„ï¼ˆå®é™…åº”ç”¨ä¸­åº”è¯¥ä½¿ç”¨çœŸå®çš„éŸ³é¢‘ç”Ÿæˆï¼‰
            temp_dir = tempfile.gettempdir()
            audio_filename = f"audio_{audio_seed or 'temp'}.wav"
            audio_path = os.path.join(temp_dir, audio_filename)
            
            # æ¨¡æ‹ŸéŸ³é¢‘ç”Ÿæˆè¿‡ç¨‹
            # åœ¨å®é™…åº”ç”¨ä¸­ï¼Œè¿™é‡Œåº”è¯¥è°ƒç”¨éŸ³é¢‘ç”Ÿæˆæ¨¡å‹
            print(f"Generating audio for prompt: {audio_prompt}...")
            
            return {
                "audio_path": audio_path,
                "success": True,
                "message": f"Audio generated successfully at {audio_path}"
            }

        except Exception as e:
            print(f"Error generating audio: {e}")
            return {
                "audio_path": "",
                "success": False,
                "message": f"Error generating audio: {str(e)}"
            }
